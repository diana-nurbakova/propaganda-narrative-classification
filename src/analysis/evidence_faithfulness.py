#!/usr/bin/env python3
"""
Evidence Faithfulness Evaluation.

Evaluates whether evidence_quote fields generated by the LLM classification
pipeline are faithful to the source documents (verbatim quotes vs paraphrases
vs hallucinations).

Reads evidence JSON sidecars from experiment runs and compares each quote
against the source document text.

Classification categories:
  - exact:      quote appears verbatim as substring of source text
  - near_exact: quote matches after whitespace/punctuation normalization (ratio >= 0.95)
  - fuzzy:      high similarity to some window of source text (ratio >= 0.80)
  - hallucination: no close match found (ratio < 0.80)
  - empty:      evidence_quote is empty, missing, or "N/A"

Usage:
    python evidence_faithfulness.py \\
        --experiments-dir ../../results/experiments/ \\
        --data-dir ../../data/dev-documents_4_December/ \\
        --output ../../results/analysis/evidence_faithfulness_report.md

    # Single experiment
    python evidence_faithfulness.py \\
        --experiments-dir ../../results/experiments/actor_critic_deepseek_en_t00_evidence/ \\
        --data-dir ../../data/dev-documents_4_December/ \\
        --output ../../results/analysis/evidence_faithfulness_report.md
"""

import argparse
import json
import os
import re
import sys
from collections import defaultdict
from datetime import datetime
from difflib import SequenceMatcher
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple


# ---------------------------------------------------------------------------
# Text normalization
# ---------------------------------------------------------------------------

def normalize_text(text: str) -> str:
    """Normalize text for comparison: collapse whitespace, strip, lowercase."""
    text = text.strip()
    text = re.sub(r"\s+", " ", text)
    return text.lower()


def normalize_light(text: str) -> str:
    """Light normalization: collapse whitespace only (preserve case)."""
    text = text.strip()
    return re.sub(r"\s+", " ", text)


# ---------------------------------------------------------------------------
# Quote matching
# ---------------------------------------------------------------------------

def classify_quote(quote: str, source_text: str) -> Tuple[str, float]:
    """
    Classify a single evidence quote against the source document.

    Returns:
        Tuple of (category, best_ratio) where category is one of:
        "exact", "near_exact", "fuzzy", "hallucination", "empty"
    """
    if not quote or quote.strip() in ("", "N/A", "n/a", "None", "none"):
        return "empty", 0.0

    # Check exact substring match (raw)
    if quote in source_text:
        return "exact", 1.0

    # Normalize and check again
    norm_quote = normalize_text(quote)
    norm_source = normalize_text(source_text)

    if not norm_quote:
        return "empty", 0.0

    if norm_quote in norm_source:
        return "near_exact", 1.0

    # Sliding window fuzzy matching
    best_ratio = _best_window_ratio(norm_quote, norm_source)

    if best_ratio >= 0.95:
        return "near_exact", best_ratio
    elif best_ratio >= 0.80:
        return "fuzzy", best_ratio
    else:
        return "hallucination", best_ratio


def _best_window_ratio(quote: str, source: str) -> float:
    """
    Slide a window over the source and find the best SequenceMatcher ratio.

    Window sizes range from 80% to 120% of quote length to allow for minor
    length differences.
    """
    q_len = len(quote)
    s_len = len(source)

    if q_len == 0:
        return 0.0
    if q_len > s_len:
        # Quote longer than source â€” compare full texts
        return SequenceMatcher(None, quote, source).ratio()

    min_win = max(1, int(q_len * 0.8))
    max_win = min(s_len, int(q_len * 1.2))

    best = 0.0

    # Step size: skip every few chars for long texts to keep runtime reasonable
    step = max(1, q_len // 10)

    for win_size in (q_len, min_win, max_win):
        if win_size > s_len:
            continue
        for start in range(0, s_len - win_size + 1, step):
            window = source[start : start + win_size]
            ratio = SequenceMatcher(None, quote, window).ratio()
            if ratio > best:
                best = ratio
                if best >= 0.95:
                    return best  # early exit

    return best


# ---------------------------------------------------------------------------
# Experiment discovery
# ---------------------------------------------------------------------------

def find_evidence_dirs(experiments_dir: str) -> List[Dict[str, Any]]:
    """
    Find all experiment runs that have evidence/ directories.

    Returns list of dicts with keys: experiment_id, run_dir, evidence_dir, config.
    """
    results = []
    exp_root = Path(experiments_dir)

    if not exp_root.exists():
        print(f"Error: experiments dir not found: {experiments_dir}")
        return results

    # Check if this is a single experiment dir (has run_* subdirs)
    run_dirs = sorted(exp_root.glob("run_*/evidence"))
    if run_dirs:
        # Single experiment directory
        for ev_dir in run_dirs:
            run_dir = ev_dir.parent
            results.append({
                "experiment_id": exp_root.name,
                "run_dir": str(run_dir),
                "evidence_dir": str(ev_dir),
            })
        return results

    # Multi-experiment directory
    for exp_dir in sorted(exp_root.iterdir()):
        if not exp_dir.is_dir():
            continue
        for ev_dir in sorted(exp_dir.glob("run_*/evidence")):
            run_dir = ev_dir.parent
            results.append({
                "experiment_id": exp_dir.name,
                "run_dir": str(run_dir),
                "evidence_dir": str(ev_dir),
            })

    return results


def detect_language(experiment_id: str) -> Optional[str]:
    """Extract language from experiment_id (e.g., 'actor_critic_deepseek_en_t00' -> 'EN')."""
    match = re.search(r"_(en|bg|hi|pt|ru)_", experiment_id, re.I)
    if match:
        return match.group(1).upper()
    return None


def load_source_document(data_dir: str, language: str, file_id: str) -> Optional[str]:
    """Load source document text."""
    doc_path = Path(data_dir) / language / "subtask-2-documents" / file_id
    if doc_path.exists():
        with open(doc_path, "r", encoding="utf-8") as f:
            return f.read()
    return None


# ---------------------------------------------------------------------------
# Evaluation
# ---------------------------------------------------------------------------

def evaluate_evidence_dir(
    evidence_dir: str, data_dir: str, language: str
) -> Dict[str, Any]:
    """
    Evaluate all evidence files in a single run's evidence/ directory.

    Returns dict with summary stats and per-document details.
    """
    ev_path = Path(evidence_dir)
    counts = defaultdict(int)
    per_document = {}
    all_details = []

    for json_file in sorted(ev_path.glob("*.json")):
        with open(json_file, "r", encoding="utf-8") as f:
            record = json.load(f)

        file_id = record.get("file_id", json_file.stem)
        source_text = load_source_document(data_dir, language, file_id)

        if source_text is None:
            continue

        doc_details = {"narratives": [], "subnarratives": []}

        # Evaluate narrative evidence quotes
        for narr in record.get("narratives", []):
            quote = narr.get("evidence_quote", "")
            category, ratio = classify_quote(quote, source_text)
            counts[category] += 1
            counts["total"] += 1
            doc_details["narratives"].append({
                "label": narr.get("narrative_name", ""),
                "quote": quote[:100] + "..." if len(quote) > 100 else quote,
                "category": category,
                "ratio": round(ratio, 3),
            })

        # Evaluate subnarrative evidence quotes
        for sub in record.get("subnarratives", []):
            quote = sub.get("evidence_quote", "")
            category, ratio = classify_quote(quote, source_text)
            counts[category] += 1
            counts["total"] += 1
            doc_details["subnarratives"].append({
                "label": sub.get("subnarrative_name", ""),
                "quote": quote[:100] + "..." if len(quote) > 100 else quote,
                "category": category,
                "ratio": round(ratio, 3),
            })

        per_document[file_id] = doc_details

    total = counts["total"]
    faithful = counts["exact"] + counts["near_exact"] + counts["fuzzy"]

    summary = {
        "total_quotes": total,
        "exact": counts["exact"],
        "near_exact": counts["near_exact"],
        "fuzzy": counts["fuzzy"],
        "hallucination": counts["hallucination"],
        "empty": counts["empty"],
        "faithfulness_rate": round(faithful / total, 4) if total > 0 else 0.0,
    }

    return {"summary": summary, "per_document": per_document}


# ---------------------------------------------------------------------------
# Report generation
# ---------------------------------------------------------------------------

def generate_report(
    all_results: List[Dict[str, Any]],
) -> str:
    """Generate Markdown report across all evaluated experiments."""
    report = "# Evidence Faithfulness Report\n\n"
    report += f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

    report += (
        "Evaluates whether `evidence_quote` fields from LLM classification output "
        "are faithful to the source documents.\n\n"
        "| Category | Description |\n"
        "|----------|-------------|\n"
        "| Exact | Quote appears verbatim in source text |\n"
        "| Near-exact | Matches after whitespace normalization (ratio >= 0.95) |\n"
        "| Fuzzy | High similarity to source window (ratio >= 0.80) |\n"
        "| Hallucination | No close match found (ratio < 0.80) |\n"
        "| Empty | Quote is empty, missing, or 'N/A' |\n\n"
    )

    # Summary table
    report += "## Summary\n\n"
    report += (
        "| Experiment | Run | Quotes | Exact | Near-exact | Fuzzy | "
        "Halluc. | Empty | Faithfulness |\n"
        "|------------|-----|--------|-------|------------|-------|"
        "---------|-------|--------------|\n"
    )

    for res in all_results:
        s = res["summary"]
        total = s["total_quotes"]
        if total == 0:
            continue

        def _pct(n):
            return f"{n} ({100*n/total:.0f}%)" if total > 0 else "0"

        run_name = Path(res["run_dir"]).name
        report += (
            f"| {res['experiment_id']} "
            f"| {run_name} "
            f"| {total} "
            f"| {_pct(s['exact'])} "
            f"| {_pct(s['near_exact'])} "
            f"| {_pct(s['fuzzy'])} "
            f"| {_pct(s['hallucination'])} "
            f"| {_pct(s['empty'])} "
            f"| {s['faithfulness_rate']:.1%} "
            f"|\n"
        )

    report += "\n"

    # Aggregate by experiment (across runs)
    by_experiment = defaultdict(lambda: defaultdict(int))
    for res in all_results:
        exp_id = res["experiment_id"]
        s = res["summary"]
        for key in ("total_quotes", "exact", "near_exact", "fuzzy", "hallucination", "empty"):
            by_experiment[exp_id][key] += s[key]

    if len(by_experiment) > 1:
        report += "## Aggregated by Experiment\n\n"
        report += (
            "| Experiment | Quotes | Exact % | Near-exact % | Fuzzy % | "
            "Halluc. % | Empty % | Faithfulness |\n"
            "|------------|--------|---------|--------------|---------|"
            "-----------|---------|-------------|\n"
        )

        for exp_id, agg in sorted(by_experiment.items()):
            total = agg["total_quotes"]
            if total == 0:
                continue
            faithful = agg["exact"] + agg["near_exact"] + agg["fuzzy"]
            report += (
                f"| {exp_id} "
                f"| {total} "
                f"| {100*agg['exact']/total:.1f}% "
                f"| {100*agg['near_exact']/total:.1f}% "
                f"| {100*agg['fuzzy']/total:.1f}% "
                f"| {100*agg['hallucination']/total:.1f}% "
                f"| {100*agg['empty']/total:.1f}% "
                f"| {100*faithful/total:.1f}% "
                f"|\n"
            )

        report += "\n"

    # Hallucination examples (most informative for debugging)
    report += "## Hallucination Examples\n\n"
    report += "Showing up to 10 hallucinated quotes with their match ratios.\n\n"

    examples = []
    for res in all_results:
        for file_id, doc in res.get("per_document", {}).items():
            for group in ("narratives", "subnarratives"):
                for item in doc.get(group, []):
                    if item["category"] == "hallucination":
                        examples.append({
                            "experiment": res["experiment_id"],
                            "file": file_id,
                            "label": item["label"],
                            "quote": item["quote"],
                            "ratio": item["ratio"],
                        })

    examples.sort(key=lambda x: x["ratio"])
    for ex in examples[:10]:
        report += (
            f"- **{ex['experiment']}** | `{ex['file']}` | {ex['label']}\n"
            f"  Quote: \"{ex['quote']}\" (ratio: {ex['ratio']:.3f})\n\n"
        )

    if not examples:
        report += "No hallucinations found.\n\n"

    return report


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Evaluate evidence faithfulness of LLM classification output",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        "--experiments-dir",
        type=str,
        default="results/experiments/",
        help="Root directory containing experiment subdirectories",
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        default="data/dev-documents_4_December",
        help="Directory containing per-language source documents",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="results/analysis/evidence_faithfulness_report.md",
        help="Output Markdown report path",
    )
    parser.add_argument(
        "--json-output",
        type=str,
        help="Optional: save raw evaluation data as JSON",
    )

    args = parser.parse_args()

    # Find evidence directories
    print("Scanning for evidence directories...")
    evidence_runs = find_evidence_dirs(args.experiments_dir)
    print(f"Found {len(evidence_runs)} runs with evidence data")

    if not evidence_runs:
        print("No evidence data found. Run experiments with evidence saving enabled first.")
        return 1

    # Evaluate each run
    all_results = []
    for i, run in enumerate(evidence_runs, 1):
        exp_id = run["experiment_id"]
        lang = detect_language(exp_id)
        if not lang:
            print(f"  [{i}/{len(evidence_runs)}] {exp_id}: could not detect language, skipping")
            continue

        print(f"  [{i}/{len(evidence_runs)}] {exp_id} ({lang})...", end=" ")
        result = evaluate_evidence_dir(run["evidence_dir"], args.data_dir, lang)
        result["experiment_id"] = exp_id
        result["run_dir"] = run["run_dir"]
        result["language"] = lang

        s = result["summary"]
        print(
            f"{s['total_quotes']} quotes | "
            f"exact={s['exact']} near={s['near_exact']} fuzzy={s['fuzzy']} "
            f"halluc={s['hallucination']} empty={s['empty']} | "
            f"faithfulness={s['faithfulness_rate']:.1%}"
        )

        all_results.append(result)

        # Save per-run JSON
        run_report_path = Path(run["run_dir"]) / "evidence_report.json"
        with open(run_report_path, "w", encoding="utf-8") as f:
            json.dump({"summary": result["summary"], "per_document": result["per_document"]},
                      f, indent=2, ensure_ascii=False)

    if not all_results:
        print("No results to report.")
        return 1

    # Generate report
    print("\nGenerating report...")
    report = generate_report(all_results)

    os.makedirs(os.path.dirname(args.output) or ".", exist_ok=True)
    with open(args.output, "w", encoding="utf-8") as f:
        f.write(report)
    print(f"Report saved to: {args.output}")

    # Save aggregate JSON
    if args.json_output:
        json_data = {
            "generated_at": datetime.now().isoformat(),
            "n_runs": len(all_results),
            "runs": [
                {
                    "experiment_id": r["experiment_id"],
                    "run_dir": r["run_dir"],
                    "language": r["language"],
                    "summary": r["summary"],
                }
                for r in all_results
            ],
        }
        os.makedirs(os.path.dirname(args.json_output) or ".", exist_ok=True)
        with open(args.json_output, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=2)
        print(f"JSON data saved to: {args.json_output}")

    print("\nDone!")
    return 0


if __name__ == "__main__":
    sys.exit(main())
