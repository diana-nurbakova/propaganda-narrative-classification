{
  "config": {
    "model_name": "mistralai:mistral-large-latest",
    "fuzzy_threshold": 70.0,
    "experiment_id": "agora_mistral_ru_t00",
    "run_id": 5
  },
  "narrative_matching": {
    "total": 4,
    "matched": 4,
    "unmatched": 0,
    "match_rate": 1.0
  },
  "narrative_matches": [
    {
      "file_id": "RU-URW-1069.txt",
      "label_type": "narrative",
      "original_label": "Praise of Russia",
      "matched_label": "URW: Praise of Russia",
      "score": 100.0,
      "matched": true
    },
    {
      "file_id": "RU-URW-1069.txt",
      "label_type": "narrative",
      "original_label": "Discrediting Ukraine",
      "matched_label": "URW: Discrediting Ukraine",
      "score": 100.0,
      "matched": true
    },
    {
      "file_id": "RU-URW-1085.txt",
      "label_type": "narrative",
      "original_label": "Praise of Russia",
      "matched_label": "URW: Praise of Russia",
      "score": 100.0,
      "matched": true
    },
    {
      "file_id": "RU-URW-1085.txt",
      "label_type": "narrative",
      "original_label": "Praise of Russia",
      "matched_label": "URW: Praise of Russia",
      "score": 100.0,
      "matched": true
    }
  ],
  "subnarrative_matching": {
    "total": 0,
    "matched": 0,
    "unmatched": 0,
    "match_rate": 0
  },
  "subnarrative_matches": []
}