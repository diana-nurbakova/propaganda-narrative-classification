# Experiment Configuration
# Model: GPT-5-nano
# Method: Naive Baseline (single agent, no validation)
# Language: EN
# Temperature: 0.0 (deterministic)
#
# Use with run_multi_experiment.py for statistical testing

model_name: "openai:gpt-5-nano"
input_folder: "data/dev-documents_4_December/EN/subtask-2-documents/"
output_file: "results/experiments/{experiment_id}/run_{run_id}/results.txt"
# Gold labels: data/dev-documents_4_December/EN/subtask-3-dominant-narratives.txt

# Model parameters for fair comparison
temperature: 0.0
top_p: 1.0
max_tokens: 16384  # High value for reasoning models with internal chain-of-thought
# seed is set per-run by run_multi_experiment.py

# Method-specific settings (Baseline: single agent, no validation)
num_narrative_agents: 1
num_subnarrative_agents: 1
narrative_aggregation_method: "union"
subnarrative_aggregation_method: "union"
enable_validation: false
enable_narrative_validation: false
enable_subnarrative_validation: false

# Standard settings
enable_cleaning: true
enable_text_cleaning: false
max_concurrency: 10

# Cost tracking
enable_cost_tracking: true
